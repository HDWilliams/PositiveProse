{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PositiveProseV1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HDWilliams/PositiveProse/blob/master/PositiveProseV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKA8PZhg684w",
        "colab_type": "code",
        "outputId": "59e7e131-6ff6-4223-be2c-1bd95f6a24d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5bb28c60a2b6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print kill\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(kill)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3AYiuCzDdV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Web Scraper for Poetry Foundations.Org\n",
        "# Instructions \n",
        "# 1. Go to https://www.poetryfoundation.org/poems/browse\n",
        "# 2. Choose your Filters\n",
        "# 3. Copy the url. That will be your parameter for the function\n",
        "\n",
        "# Here is an example of me scraping poems based\n",
        "# off a TOPIC filter of \"Love\" and a SCHOOL/PERIOD filter of 1951-Present\n",
        "\n",
        "# You will modify these in order to control the parameters of the script!\n",
        "BROWSE_FIRST_URL = \"https://www.poetryfoundation.org/poems/browse#page=1&sort_by=recently_added&topics=20&school-period=1951-present\"\n",
        "JUST_TESTING = False # set to false once you actually decide to parse all the results\n",
        "\n",
        "from google.colab import files\n",
        "import requests\n",
        "import json\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "\n",
        "# set request headers to get around bot blocking technology\n",
        "\n",
        "s = requests.session()\n",
        "s.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'})\n",
        "\n",
        "def main():\n",
        "  scrape_poems(BROWSE_FIRST_URL)\n",
        "  \n",
        "\n",
        "def scrape_poems(url):\n",
        "  print(\"Scraping Poems...\")\n",
        "  # A default id meant for testing if you want to comment out parse_ids\n",
        "  ids = [149702]\n",
        "  \n",
        "  # scrape\n",
        "  ids = parse_ids(url)\n",
        "  poems = parse_poems(ids)\n",
        "  \n",
        "  # convert to json and save off\n",
        "  \n",
        "  filename = \"poems.json\"\n",
        "  with open(filename, \"w\") as outfile:\n",
        "    json.dump(poems,outfile)\n",
        "    \n",
        "\n",
        "# get all the IDs for all the poems of the selected filters\n",
        "def parse_ids(url):\n",
        "  id_arr = []\n",
        "  # convert url into request URL to get a JSON of Poems\n",
        "  # Example of a request URL https://www.poetryfoundation.org/ajax/poems?page=1&sort_by=recently_added&topics=20&school-period=1951-present\n",
        "  request_url=url.replace(\"poems/browse#\",\"ajax/poems?\")\n",
        "  response=s.get(request_url)\n",
        "  response_json = response.json()\n",
        "\n",
        "  # there are 20 results per page\n",
        "  total_poems = response_json[\"TotalResults\"]\n",
        "  num_pages = ceil(total_poems / 20)\n",
        "\n",
        "  # keep max pages for testing to reduce load\n",
        "  # test 2 pages to ensure this is actually working\n",
        "  if JUST_TESTING:\n",
        "    num_pages = 1\n",
        "\n",
        "  for page_num in range(1, num_pages + 1):\n",
        "    old_page_str = \"page=\" + str(page_num - 1)\n",
        "    new_page_str = \"page=\" + str(page_num)\n",
        "    request_url = request_url.replace(old_page_str,new_page_str)\n",
        "\n",
        "    response=s.get(request_url)\n",
        "    poem_entries = response.json()[\"Entries\"]\n",
        "    entries_ids = [entry[\"id\"] for entry in poem_entries]\n",
        "    id_arr.extend(entries_ids)\n",
        "\n",
        "  # verify that the number of ids is equal to what we were promised\n",
        "  print(\"Obtained \" + str(len(id_arr)) + \" poem ids of total \" + str(total_poems))\n",
        "  return id_arr\n",
        "\n",
        "# access every poem and scrape the poetry content\n",
        "def parse_poems(id_arr):\n",
        "  count = 1\n",
        "  base_url = \"https://www.poetryfoundation.org/poems/\"\n",
        "  poems = {}\n",
        "  print(\"Parsing poem \"),\n",
        "  for id in id_arr:\n",
        "    print(count),\n",
        "    count = count+1\n",
        "    poem_url = base_url + str(id)\n",
        "    \n",
        "    \n",
        "    #we dont care about what's happening so just skip failures\n",
        "    try:\n",
        "      response = s.get(poem_url, allow_redirects=True)\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "      title_box = soup.find(\"div\", attrs={\"class\":\"c-feature-hd\"})\n",
        "      title = title_box.text.strip()\n",
        "\n",
        "      poem_box = soup.find(\"div\", attrs={\"class\":\"o-poem\"})\n",
        "\n",
        "      #Rohun look at what happens if I print poem_box.text\n",
        "      poems[title] = poem_box.text.strip()\n",
        "\n",
        "      # NOTE: IF YOU CHOOSE TO UNCOMMENT THESE PRINT STATEMENTS \n",
        "      # THEN PLEASE COMMENT OUT THE FUNCTION CALL TO parse_id\n",
        "\n",
        "      # print(title + \" : \" + poems[title])\n",
        "      # print(title)\n",
        "      # print(poems)\n",
        "    except:\n",
        "      print(\"Failure on poem \" + str(id) + \". Continuing....\")\n",
        "    \n",
        "  return poems\n",
        "    \n",
        "if __name__ == '__main__': main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DUyAJKZaFg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/gdrive/My\\ Drive/foo.txt\n",
        "\n",
        "#Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
        "#Enter your authorization code:\n",
        "#··········\n",
        "#Mounted at /content/gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WGS5Wet6xKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POEM CLEANUP AND OTHER PRE-PROCESSING\n",
        "\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "pattern = re.compile('([^\\s\\w]|_)+') \n",
        "\n",
        "# clean Poems of weird characters\n",
        "filename = \"poems.json\"\n",
        "newFilename = \"compiledPoems.json\"\n",
        "new_data = {}\n",
        "\n",
        "with open(filename, \"r\") as infile:\n",
        "  data = json.load(infile)\n",
        " \n",
        "for title, poem_string in data.items():\n",
        "  strippedPoem = re.sub('[^A-Za-z0-9\" \"]+', '', poem_string)\n",
        "  strippedTitle = re.sub('[^A-Za-z0-9\" \"]+', '', title)\n",
        "  new_data[strippedTitle]=strippedPoem\n",
        "\n",
        "with open(newFilename, \"w\") as outfile:\n",
        "  json.dump(new_data,outfile)\n",
        " \n",
        "files.download(newFilename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wRBWzsSMIpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "''' Continuous bag-of-words model for word2vec.\n",
        "Parameters:\n",
        "    vocab_size: number of defined words in the vocab\n",
        "    embedding_dim: desired embedded vector dimension\n",
        "    context_size: number of context words used\n",
        "'''\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n",
        "        out = self.linear(embeds)\n",
        "        log_probs = F.log_softmax(out)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "''' Skip-gram bag-of-words model for word2vec.\n",
        "Parameters:\n",
        "    vocab_size: number of defined words in the vocab\n",
        "    embedding_dim: desired embedded vector dimension\n",
        "'''\n",
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = self.linear(embeds)\n",
        "        log_probs = F.log_softmax(out)\n",
        "        return log_probs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9kwIx7SMNux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### example of running Skipgram model embeddings\n",
        "loss_function = nn.NLLLoss()\n",
        "model = SkipGram(len(vocab), EMBEDDING_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Starting training\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = torch.Tensor([0])\n",
        "    print(\"Beginning epoch %d\" % epoch)\n",
        "    progress_bar = progressbar.ProgressBar()\n",
        "    for context, target in progress_bar(training_data):\n",
        "        context_var = autograd.Variable(torch.LongTensor(context))\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_var)\n",
        "        loss = loss_function(log_probs, autograd.Variable(\n",
        "            torch.LongTensor([target])))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "    print(\"Epoch %d Loss: %.5f\" % (epoch, total_loss[0]))\n",
        "    losses.append(total_loss)\n",
        "\n",
        "# Visualize embeddings\n",
        "if EMBEDDING_DIM == 2:\n",
        "    indices = np.random.choice(np.arange(len(vocab)), size=10, replace=False)\n",
        "    for ind in indices:\n",
        "        word = list(vocab.keys())[ind]\n",
        "        input = autograd.Variable(torch.LongTensor([word_to_ix[word]]))\n",
        "        vec = model.embeddings(input).data[0]\n",
        "        x, y = vec[0], vec[1]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, xy=(x, y), xytext=(5, 2),\n",
        "                     textcoords='offset points', ha='right', va='bottom')\n",
        "    plt.savefig(\"w2v.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKaRCSSlMSLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Long Short Term Memory RNN Model\n",
        "#One input layer, two hidden layers, connected to a Linear fully connected layer as the output\n",
        "#have the option to use drop out, and bidirectional LSTM with dropout\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__ (self, input_dim, hidden_dim, output_dim, num_layers, batch_size, W2V_model, droprate = .2, nonlin='relu', bidirect=False):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.W2V = W2V_model\n",
        "    \n",
        "    #define LSTM module (two layers)\n",
        "    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, dropout=droprate, nonlinearity=nonlin, bidirectional=bidirect)\n",
        "    \n",
        "    #define fully connected output layer\n",
        "    self.linear = nn.Linear(self.hidden_layer, self.output_dim)\n",
        "    \n",
        "    #initialize the hidden layer with random weights\n",
        "    def init_hidden_state(self):\n",
        "      return (torch.rand(self.num_layers, self.batch_size, self.hidden_dim), torch.rand(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "    \n",
        "    #define forward pass through the model\n",
        "    def forward(self, init_input, hidden, ):\n",
        "      #get output of lstm and then pass it to fully connected layer\n",
        "      vec_words = self.W2V(init_input)\n",
        "      lstm_output, self.hidden = self.lstm(vec_words, hidden)\n",
        "      y_pred = self.linear(lstm_output[-1].view(self.batch_size, -1))\n",
        "      return y_pred.view(-1)\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}